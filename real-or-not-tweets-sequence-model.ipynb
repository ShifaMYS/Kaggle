{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel demonstrates binary classification of tweets using Sequence model.\n\n\nLet's start with importing all the necessary packages."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom collections import Counter, namedtuple\n\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, accuracy_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Embedding\nfrom keras.optimizers import Nadam,adam\n\nnp.random.seed(1)","execution_count":103,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/nlp-getting-started/train.csv')","execution_count":58,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's see how data looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":59,"outputs":[{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Check for class imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns = ['id','keyword','location'], inplace=True)\nneg, pos = np.bincount(data.target)\nprint(f'Total: {len(data)} \\nPositive: {pos} \\nNegative: {neg}')","execution_count":60,"outputs":[{"output_type":"stream","text":"Total: 7613 \nPositive: 3271 \nNegative: 4342\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"There is no class imbalance problem."},{"metadata":{},"cell_type":"markdown","source":"Check for null values in data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":61,"outputs":[{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"text      0\ntarget    0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Let's work with tweets\n\nClean the text by removing urls, html tags, emojis and stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \n    #remove urls\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = url_pattern.sub(r'', text)\n    \n    #remove html\n    html_pattern = re.compile(r'<.*?>')\n    text = html_pattern.sub(r'', text)\n    \n    #remove emojis\n    emoji_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    text = emoji_pattern.sub(r'',text)\n    \n    #remove punctuations\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    text = text.translate(table)\n    \n    #remove stopwords\n    stop = set(stopwords.words('english'))\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n\n    return ' '.join(text)","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: clean_text(x))","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"                                                text  target\n0       deeds reason earthquake may allah forgive us       1\n1              forest fire near la ronge sask canada       1\n2  residents asked shelter place notified officer...       1\n3  13000 people receive wildfires evacuation orde...       1\n4  got sent photo ruby alaska smoke wildfires pou...       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>deeds reason earthquake may allah forgive us</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>forest fire near la ronge sask canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>residents asked shelter place notified officer...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13000 people receive wildfires evacuation orde...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We cannot directly use textual data as input to our sequence model. We need to map each word in the tweet to an integer. We can then use Embedding layer of keras to vector encode the words.\n\nLet's find the vocabulary size first."},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_counter(text):  \n    \n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count    \n\ntext = data['text']\ncounter = word_counter(text)\n\nvocab_size = len(counter)","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to have a fixed sized input for the model, here I am using maximum length as 20. Try with different values to find the best one. Usually a smaller value is recommended since it makes the input less sparse when padded with zeros."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 20","execution_count":66,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To map the words to unique integer values, we will be using keras Tokenizer.\n\nKeras Tokenizer can be used to get the sequence for each tweet. It maps each word to an integer, representing an index of that word in word_index list."},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Tokenizer(num_words = vocab_size)\nt.fit_on_texts(data['text'])\n\nword_index = t.word_index\n\ndict(list(word_index.items())[:10])","execution_count":67,"outputs":[{"output_type":"execute_result","execution_count":67,"data":{"text/plain":"{'like': 1,\n 'im': 2,\n 'amp': 3,\n 'fire': 4,\n 'get': 5,\n 'new': 6,\n 'via': 7,\n 'people': 8,\n 'one': 9,\n 'news': 10}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We will use this tokenizer later on train and test tweets."},{"metadata":{},"cell_type":"markdown","source":"Let's take initial 7500 examples for training and validation, remaining for testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[:7500]","execution_count":147,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's build a sequential model using keras.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 200, input_length = max_len))\nmodel.add(LSTM(80))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nnadam = Nadam(learning_rate=0.0001)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer=nadam, metrics=['accuracy'])","execution_count":148,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":149,"outputs":[{"output_type":"stream","text":"Model: \"sequential_17\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_17 (Embedding)     (None, 20, 200)           3594200   \n_________________________________________________________________\nlstm_17 (LSTM)               (None, 80)                89920     \n_________________________________________________________________\ndropout_17 (Dropout)         (None, 80)                0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 81        \n=================================================================\nTotal params: 3,684,201\nTrainable params: 3,684,201\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5)\nX = df['text']\ny = df['target']","execution_count":150,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\n# train model on 5 folds\nfor train_index, test_index in skf.split(X, y):\n    \n    train_x, test_x = X[train_index], X[test_index]\n    train_y, test_y = y[train_index], y[test_index]\n    print(\"Tweet before tokenization: \", train_x.iloc[0])\n    \n    #Tokenize the tweets using tokenizer.\n    train_tweets = t.texts_to_sequences(train_x)\n    test_tweets = t.texts_to_sequences(test_x)\n    print(\"Tweet after tokenization: \", train_tweets[0])\n    \n    #pad the tokenized tweet data\n    train_tweets_padded = pad_sequences(train_tweets, maxlen=max_len, padding='post', truncating='post')\n    test_tweets_padded = pad_sequences(test_tweets, maxlen=max_len, padding='post', truncating='post')\n    print('Tweet after padding: ', train_tweets_padded[0])\n    \n    #train model on processed tweets\n    history = model.fit(train_tweets_padded, train_y, epochs=5, validation_data = (test_tweets_padded,test_y))\n    \n    #make predictions\n    pred_y = model.predict_classes(test_tweets_padded)\n    print(\"Validation accuracy : \",accuracy_score(pred_y, test_y))\n    \n    #store validation accuracy\n    accuracy.append(accuracy_score(pred_y, test_y))","execution_count":151,"outputs":[{"output_type":"stream","text":"Tweet before tokenization:  ika tuning soup diet recipes fat burning soup recipes fat burning soup diet recip\nTweet after tokenization:  [8621, 4975, 2946, 2460, 2947, 1276, 21, 2946, 2947, 1276, 21, 2946, 2460, 8622]\nTweet after padding:  [8621 4975 2946 2460 2947 1276   21 2946 2947 1276   21 2946 2460 8622\n    0    0    0    0    0    0]\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","name":"stderr"},{"output_type":"stream","text":"Train on 6000 samples, validate on 1500 samples\nEpoch 1/5\n6000/6000 [==============================] - 16s 3ms/step - loss: 0.6796 - accuracy: 0.5733 - val_loss: 0.6734 - val_accuracy: 0.5740\nEpoch 2/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.4919 - accuracy: 0.7675 - val_loss: 0.5827 - val_accuracy: 0.7120\nEpoch 3/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.2917 - accuracy: 0.8887 - val_loss: 0.6270 - val_accuracy: 0.6987\nEpoch 4/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.1914 - accuracy: 0.9375 - val_loss: 0.7297 - val_accuracy: 0.6967\nEpoch 5/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.1342 - accuracy: 0.9588 - val_loss: 0.8489 - val_accuracy: 0.6833\nValidation accuracy :  0.6833333333333333\nTweet before tokenization:  deeds reason earthquake may allah forgive us\nTweet after tokenization:  [4368, 716, 152, 54, 1454, 4369, 13]\nTweet after padding:  [4368  716  152   54 1454 4369   13    0    0    0    0    0    0    0\n    0    0    0    0    0    0]\nTrain on 6000 samples, validate on 1500 samples\nEpoch 1/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.2395 - accuracy: 0.9138 - val_loss: 0.1002 - val_accuracy: 0.9760\nEpoch 2/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.1355 - accuracy: 0.9552 - val_loss: 0.1118 - val_accuracy: 0.9653\nEpoch 3/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0944 - accuracy: 0.9690 - val_loss: 0.1305 - val_accuracy: 0.9547\nEpoch 4/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.0766 - accuracy: 0.9737 - val_loss: 0.1509 - val_accuracy: 0.9480\nEpoch 5/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0651 - accuracy: 0.9775 - val_loss: 0.1669 - val_accuracy: 0.9420\nValidation accuracy :  0.942\nTweet before tokenization:  deeds reason earthquake may allah forgive us\nTweet after tokenization:  [4368, 716, 152, 54, 1454, 4369, 13]\nTweet after padding:  [4368  716  152   54 1454 4369   13    0    0    0    0    0    0    0\n    0    0    0    0    0    0]\nTrain on 6000 samples, validate on 1500 samples\nEpoch 1/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0816 - accuracy: 0.9720 - val_loss: 0.0849 - val_accuracy: 0.9660\nEpoch 2/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.0548 - accuracy: 0.9818 - val_loss: 0.0872 - val_accuracy: 0.9613\nEpoch 3/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0474 - accuracy: 0.9838 - val_loss: 0.0860 - val_accuracy: 0.9647\nEpoch 4/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0440 - accuracy: 0.9833 - val_loss: 0.0973 - val_accuracy: 0.9647\nEpoch 5/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0400 - accuracy: 0.9842 - val_loss: 0.1006 - val_accuracy: 0.9620\nValidation accuracy :  0.962\nTweet before tokenization:  deeds reason earthquake may allah forgive us\nTweet after tokenization:  [4368, 716, 152, 54, 1454, 4369, 13]\nTweet after padding:  [4368  716  152   54 1454 4369   13    0    0    0    0    0    0    0\n    0    0    0    0    0    0]\nTrain on 6000 samples, validate on 1500 samples\nEpoch 1/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0535 - accuracy: 0.9783 - val_loss: 0.0456 - val_accuracy: 0.9847\nEpoch 2/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.0448 - accuracy: 0.9825 - val_loss: 0.0427 - val_accuracy: 0.9847\nEpoch 3/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.0430 - accuracy: 0.9812 - val_loss: 0.0416 - val_accuracy: 0.9847\nEpoch 4/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0399 - accuracy: 0.9817 - val_loss: 0.0467 - val_accuracy: 0.9847\nEpoch 5/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.0363 - accuracy: 0.9827 - val_loss: 0.0482 - val_accuracy: 0.9847\nValidation accuracy :  0.9846666666666667\nTweet before tokenization:  deeds reason earthquake may allah forgive us\nTweet after tokenization:  [4368, 716, 152, 54, 1454, 4369, 13]\nTweet after padding:  [4368  716  152   54 1454 4369   13    0    0    0    0    0    0    0\n    0    0    0    0    0    0]\nTrain on 6000 samples, validate on 1500 samples\nEpoch 1/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.0429 - accuracy: 0.9827 - val_loss: 0.0236 - val_accuracy: 0.9873\nEpoch 2/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0415 - accuracy: 0.9807 - val_loss: 0.0242 - val_accuracy: 0.9873\nEpoch 3/5\n6000/6000 [==============================] - 15s 2ms/step - loss: 0.0394 - accuracy: 0.9823 - val_loss: 0.0247 - val_accuracy: 0.9873\nEpoch 4/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.0378 - accuracy: 0.9833 - val_loss: 0.0270 - val_accuracy: 0.9873\nEpoch 5/5\n6000/6000 [==============================] - 15s 3ms/step - loss: 0.0379 - accuracy: 0.9827 - val_loss: 0.0273 - val_accuracy: 0.9873\nValidation accuracy :  0.9873333333333333\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Validation accuracy of the model :\", np.mean(accuracy))","execution_count":155,"outputs":[{"output_type":"stream","text":"Validation accuracy of the model : 0.9118666666666666\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Our model is trained with validation accuracy of 91%, let's see how it performs on unseen tweets from test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = data[7501:]\n\ntokenized_tweets = t.texts_to_sequences(test_df['text'])\npadded_tweets = pad_sequences(tokenized_tweets, maxlen=max_len, padding='post', truncating='post')\ntest_y = test_df['target']\npred_y = model.predict_classes(padded_tweets)","execution_count":153,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(pred_y, test_y)","execution_count":154,"outputs":[{"output_type":"execute_result","execution_count":154,"data":{"text/plain":"0.9285714285714286"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We acheived 92% test accuracy!!ðŸŽ‰"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"References:\n\nhttps://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n\nhttps://www.youtube.com/watch?v=j7EB7yeySDw"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}